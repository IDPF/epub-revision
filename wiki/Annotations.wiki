#summary The EPub vNext Annotations sub-group page

= Annotations - 'metadata on content' =

<wiki:toc max_depth="5" />

== Mission ==

The goal of the annotations sub-group is to provide a specification for schema and storage of data associated with content in an EPUB document. It is quite likely that the specification will be an entirely separate document (e.g. not unlike OPS, OCF, OPF) under the EPUB umbrella. (Needs refinement.)

* Slides from f2f:
  *  [http://files.me.com/wmanis/c2o1l8 Annotations.ppt]
  *  [http://files.me.com/wmanis/ce9tb1 Mei-Li Chen Institute of Information Industry]



== Process ==

  * *Requirements due end of July 2010*
  * Please use email subject line: "annot" : subject for all email communications
  * Scheduled Skype call:<br>
    * tbd

== Participants ==

|| *Name* ||  *Organization* || *Role* || *UT time* || *Preferred con call time* ||
|| Will Manis ||  Viscous Documents || (coordinator) ||  UTC/GMT -8 || late afternoon/evenings || 
||  Mei-Li Chen || Institute of Information Industry || || UTC/GMT +8 || ||
||  Cho Ching Lu ||  Institute of Information Industry || ||  UTC/GMT +8 || ||
||  King Wai Chow ||  ASTRI || || UTC/GMT +8 || 02:00 - 10:00, 13:00 - 16:00, 22:00 - 23:00UTC ||
|| Jon Noring || Invited expert || || UTC/GMT -5 || ||
|| Norm Walsh || MarkLogic Corporation || || UTC/GMT -5 || ||
|| Ben Trafford || Invited expert || || UTC/GMT -8 || 14:00 - 00:00UTC, 03:00-08:00UTC ||
|| Roger Sperberg || Invited expert || || UTC/GMT -5 || ||
|| Wen Hsuan Hsieh || Institute of Information Industry || || UTC/GMT +8 || 00:00 ~ 04:00 UTC ||
|| Keith Fahlgren || Threepress || || UTC/GMT -8 || 20:00-23:00 UTC ||
|| Chelsea Valentine || LibreDigital || || UTC/GMT -6 || 19:00-22:00 UTC ||
|| Karen Broome || Sony Electronics || || UTC/GMT -8 || ||
|| Daniel Weck || DAISY Consortium || || UTC/GMT (London) || ||
|| Takeshi Kanai || Sony Electronics || || UTC/GMT -8:00 || 1PM - 4PM ||
|| Ronnie Gambon || HarperCollins Publishers || || EST (New York) ||  ||
|| Alex Zu || ASTRI || || UTC/GMT +8 || 10am - 4pm local ||
|| name || org || role || tz || preferred time || 


== Scenarios ==

This section should have scenarios that describe the creation and use of metadata/annotations. It is not a requirement that the final specification covers every scenario. It should not go into technical details about implementation issues:
  *	Authors annotations  ancillary content included at content packaging/publication time, these may be included in the EPUB (shared)
    #	Last minute errata
    #	Pronunciation (e.g. voice/sound, phonetic (rubi, IPA, etc.)
    #	Footnote
    #	Commentary (e.g voice, text, video, etc.)
  *	Readers private annotations  annotations and bookmarks developed by a single reader for their private use
    #	Underlining/highlighting significant content (text, images, tables, video, etc.)
    #	Adding a bookmark (perhaps automatically by the reading system for the farthest position read)
    #	Adding a comment (textual note) to a range of selected content (text, image, video).
    #	Mindless drawing/doodles
    #	Adding an image to a range of selected content
    #	Adding an audio or video recording to a range of selected content
    #   Create a linkage between a point in a document and any other resource
  *	Published annotations  may be part of a commercial offering or an explicit group context (shared)
    #   All of the above
    #	Answers to a quiz (rich media) meant for teacher.
    #	Notes from a teacher to students.
    #	Notes from myself to members of my book club.
    #	Adding a table containing additional experimental data to an existing table in a scholarly journal.
    #	Annotations from a third party for sale to students reading Huckleberry Finn.
    #   Errata
    #	Definitive commentary from an interesting voice (editor, celebrity, historical player, etc).
    #   Annotations on annotations

== Requirements & issues ==

==== ISSUE_I1 How far should this specification go in suggesting how user agents display annotations? ====

_Rationale: The display of annotations can be an exceedingly complex problem, we need to allow UA freedom to handle accessibility, device limitations, domain requirements, etc. Should we focus on the data representations and integrity of annotations and a very set of minimal display requirements that ensure the integrity of the annotation, the annotator and the annotated content? Or more display requirements?

==== ANNOT_R1 Mechanism to guarantee that document content has not changed ====

_Rationale: The scenario we need to prevent here is  - an author creates a document and says Vinay is a good guy, I come along and add an annotation that says I agree (that Vinay is a good guy), the author later edits the document to read Vinay is a criminal, my I agree annotation will now be misinterpreted by subsequent viewers of my annotation.

==== ANNOT_R1.1 Mechanism to guarantee identity of document author ====
==== ANNOT_R1.2 Mechanism to guarantee anonymity of document author ====

==== ANNOT_R2 Mechanism to guarantee that the public annotation has not changed ====

_Rationale: The scenario we need to prevent here is  - an author creates a document and says Vinay is a good guy, I come along and add an annotation that says I agree, I do not want a subsequent annotator to change my annotation to "I disagree".

==== ANNOT_R3 Document identifiers ====

_Rationale: Annotations stored seperately from the referenced document should be able to reliably identify the referenced document.

==== ANNOT_R4 Content identification ====

_Rationale: Annotations must be able to reference text, audio, video, spatial (ink circle of a face in an image), etc. 

- 2010/07/16, Daniel Weck: I suggest breaking-down ANNOT_R4 into several, more accurate requirements. I have drafted a few and named them "4.x", as follows:


==== ANNOT_R4.1 Identification of an annotation target as a named (i.e. predefined, authored with a unique identifier), media-agnostic structural point/location in the content document. ==== 

_Rationale: annotations must be able to reference a particular XML element in the content document, regardless of the type of content the pointed element represents (e.g. text markup, video or audio tag from HTML5...etc.), using unique names (such as XML fragment identifiers) that have been authored explicitly in the targeted content document (i.e. predefined/prepared intentionally by the content producers).

====  ANNOT_R4.2 Same as ANNOT_R4.1, but instead of targeting predefined unique identifiers, here the identification of the point/location in the content document is based on a given path leading to the XML element in question, which requires structural assumptions (i.e. the knowledge from the annotation creator about the structure of the document). ==== 

NOTE: by virtue of ANNOT_R1, the structure of the content document is considered immutable which guarantees that the annotation targets described in ANNOT_R4.1 and ANNOT_R4.2 cannot be broken.

==== ANNOT_R4.3 Identification of an annotation target as a location in between 2 consecutive unicode characters inside the XML content document. ==== 

_Rationale: annotations must be able to reference a particular point/location in a stream of unicode characters, expressed relatively to a particular structural point in the document that contains this text (ancestor/descendant relationship), such as an XML element (opening "tag") defined by the mechanisms described in ANNOT_R4.1 and ANNOT_R4.2.

==== ANNOT_R4.4 Identification of an annotation target as a range of unicode characters inside the XML content document. ==== 

_Rationale: annotations must be able to reference a particular contiguous (i.e. non-disjoint) "selection" of text, based on two points/locations specified using the mechanism described in ANNOT_R4.3.

==== ANNOT_R4.5 Identification of an annotation target as a time offset inside audio or video resources that are part of the EPUB publication. ==== 

_Rationale: annotations must be able to reference media objects contained inside the main content document or part of the synchronized audio content of the publication.

==== ANNOT_R4.6 Same as ANNOT_R4.5, but for a range of time (i.e. a particular "clip" of the media object) defined by a begin and an end time offset relative to the starting point of the media object. ==== 

==== ANNOT_R4.7 Identification of an annotation target as a spacial area in media objects inside the content document ==== 

_Rationale: annotations must be able to reference a particular graphical zone/region of the target visual media (i.e. image, video), based on a specified geometrical shape and associated coordinates in the space domain (e.g. rectangle, circle, arbitrary outline).

==== ANNOT_R4.8 Identification of an annotation target can mix the structural, spacial and time domains. ====

_Rationale: annotations (particularly the ones addressing video objects) must be able to reference a specific graphical area for the duration of a specific time range in the target media object.

==== ANNOT_R4.9 Annotation may be on multiple selection targets  ====

_Rationale: "There are 7 misspelled instances of the word 'accessability'"


==== ANNOT_R5 Content identification resilient to reflow ====

_Rationale: The content identification scheme must be resilient to content reflow.

==== ANNOT_R6 Annotator identification ====

_Rationale: The annotator should be able to be reliably identified if they so desire. 

==== ANNOT_R7 Annotator anonymity ====

_Rationale: In the event the annotator does not want to be identified the annotation should contain no PI.


==== ANNOT_R8 Annotation content can contain either/any of: audio, video, image, text.  ====

_Rationale: annotations must be able to contain multimedia objects, TO BE DEFINED: can one given annotation contain more than one object, such as a mix of video and images for example ? If yes, are there any presentation requirements for these mixed media objects, or should the user-agent be responsible for orchestrating the presentation flow of these multiple media items ?


==== ANNOT_R9 Synchronized audio annotates the text====
_Use case: A audio book producer wants to add a narration to the EPUB text version. They want synchronization to take place at various levels of granularity, i.e. paragraph, sentence, word.

_Use case 2: A EPUB document contains poetry and somebody wants to add a human reading of the poem. Different human presentations can be compared, which interprets the meaning of the poem in different ways. e.g. "this is my son" one version is with pride, the other with shame. 


== Background material ==

  * DAISY
    * Portable Bookmarks and Highlights http://www.daisy.org/z3986/2005/Z3986-2005.html#Bkmk.
  * Cathy Marshall 
    * http://www.csdl.tamu.edu/~marshall/pubs.html
    * http://research.microsoft.com/en-us/people/cathymar/
  * AJ Bernheim Brush
    * http://research.microsoft.com/en-us/people/ajbrush/ajbprojects.aspx#annotations
  * Barry Brahier
    * http://www.cehd.umn.edu/Reading/documents/reports/Brahier-report.pdf 
  * Heck, Luebke, Obermark
    * http://www.math.grin.edu/~rebelsky/Blazers/Annotations/Summer1999/Papers/survey_paper.html

== Discussion ==

== Specification ==